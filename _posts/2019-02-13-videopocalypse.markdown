---
layout: post
title:  "Videopcalypse"
date:   2019-02-02 20:58:06 +0800
categories: science fiction
---

    Roq 

    a world of facades and 

I can get a video and alter it to the point of perfection.
The perspective of a video hacker. Scene of him trying to overcome various security measures. 


Articles for evidence :

Prevention measures :
https://www.wired.com/story/amber-authenticate-video-validation-blockchain-tampering-deepfakes/
https://www.wired.com/story/the-blockchain-solution-to-our-deepfake-problems/

^^ Blockchain as the solution. Every 30 seconds that a video is being taken, its state is uploaded to the blockchain after being hashed by a unique algorithm only known to the hasher. While changes to the original is possible, it'll be near impossible to deviate significantly, since the video hacker would only have a 30 second interval before the original would have to match the blockchain-uploaded version. 

I can see this being used for police body cams and other high stake recordings, but will it reach the mainstream? And even if it does, what's preventing deepfakers from synthesizing a completely new fake video based loosely on a real one, then uploading it to the blockchain, cementing its authenticity? And there'll be quite some time before blockchain connected devices are distributed widely in infrastructure; before then, as long as a room lacks one of these devices, reality would still be distortable. 


https://www.wired.com/story/face-swap-porn-legal-limbo/
1. Biggest thing that lawyers have against deepfakes is that they are a privacy violation, but it's technically not - what's shown isn't the actual steamy scenes insides celebrities' sex lives, but recreations of them.
2. Taking deepfakes down may be considered as censorship - a stifling of the inspired voices of sexually liberal artists.
3. A point of attack could be the Federal Trade Commission Act's prohibition of â€œunfair or deceptive acts or practices in or affecting commerce". Welding the faces and bodies of different people without their consent *may* be a deceptive act in the porn industry.

https://www.wired.com/story/why-artificial-intelligence-researchers-should-be-more-paranoid/
Being less open about research on artificial intelligence may give the "white hats" a head start, but aren't the benefits of open source too big to ignore? Oftentimes huge improvements to the code are made by the community, which is a direct result of its ability to download all the code and play around with it freely (improvements not only in the code's effectiveness but in methods to defend against it). And if it's only a matter of time before the cyber criminals catch up to the researchers and start circulating fakes, then won't it be better to have a community of coders well informed of the intricacies of the code's inner workings so that they can easily identify its products? 

To use a slightly convoluted analogy : in scenario number one, the instructions for crafting super-guns are left in the open for both enemy and fellow knights to see; the knights study the gun's anatomy and search for any vulnerabilities, such that when they do get attacked by the gun, they know how to minimize the damage. Scenario number two : the king hides this magic formula in his chest, an enemy burglar steals it, and later, in war, the knights are taken by surprise and die in confused helplessness.

I assume the burglar's heist is successful, and this may be where my argument falls apart. If advanced image and voice synthesis can only be achieved after huge amounts of specialized investments, and the discovered techniques are so crucial to the process that without them hackers are rendered powerless, then yes, keeping AI research from the hands of the public might be the correct way to go. 

If not - keep the gates open, Google, and keep feeding us your brilliant material.




How might deepfakes be punished? Technically, the celebrities whose faces are used in deepfakes cannot claim that they have been "harmed" in any way - current technologies are still crude enough that tampered videos can be easily identified (hence throwing defamation claims out the window). But in 20, 30 years, who knows how fast AI might have progressed? The deepfakers might win our best computer scientists, and distribute fake porn that is unidentifiable as such; at that point, the legalities become unambiguous - the creators should be punished. Problem is, can they? The moment deepfakes become so real that the creators are incriminated,they also become indistinguishable from real videos, leaving prosecutors with no real evidence to charge the deepfakers. 


An exploration of the meaning of truth in an age when it is falling apart.